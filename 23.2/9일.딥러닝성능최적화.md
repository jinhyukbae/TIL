# 딥러닝 성능 최적화

## 데이터를 사용한 성능 최적화

### 많은 데이터 수집
* 딥러닝,머신러닝은 일반적으로 데이터양이 많을 수록 좋다.

### 임의 데이터 생성
* 데이터 범위 (scale) 조정
  * 시그모이드 활성화 함수 사용시 데이터셋 범위를 0~1 사이가 되도록 함 (정규분포를 따르게 함)
  * tanh 함수 사용시 데이터셋 범위를 1 ~ -1 사이가 되도록 함

* 정규화,규제화,표준화

## 알고리즘을 이용한 성능 최적화

* 비슷한 용도의 알고리즘들을 선택하여 모델 훈련후 최적의 성능을 보이는 알고리즘 선택
  * 데이터 분류 : SVM, KNN 등
  * 시계열 데이터 : RNN, LSTM, GRU 등
 
## 알고리즘 튜닝을 통한 성능 최적화
* 성능 최적화에 가장 많은 시간 소요
* 다양한 파라미터를 변경하면서 훈련시키고 최적의 성능 도출
 
### 선택 가능한 하이퍼파라미터
* 진단
  * 모델에 대한 평가를 바탕으로 모델이 OVER FITTING 인지 다른 원인인지 통찰 가능
  * 훈련 성능이 검증 보다 눈에 띄게 좋다면 과적합 의심 규제화로 성능 향상
  * 훈련, 검증 결과 모두 좋지 않다면 과소적합 의심 네트워크 구조 변경 또는 EPOCH 수 조정

* 가중치
  * 가중치 초기값을 작은 난수를 사용하여 사전 훈련(오토인코더 같은 비지도 학습을 이용하여 가중치 정보를 얻기 위한 사전 훈련)을 진행한 후 지도 학습을 진행

* 학습률
  * 모델의 네트워크 구성에 따라 다르므로 초기에 매우 크거나 작은 난수를 선택하여 학습 결과를 관찰하면서 조금씩 변경
  * 네트워크 계층이 많으면 학습률을 높게, 적으면 학습률을 작게 설정

* 활성화 함수
  * 활성화 함수 변경시 손실 함수도 함께 변경해야 하는 경우가 많으므로 신중하게 한다.
  * 활성화 함수로 시그모이드, tanh 함수 사용했을 경우 출력층에서는 소프트맥스 또는 시그모이드 함수를 많이 선택한다.

* batch와 epoch
  * 일반적으로 큰 에포크와 작은 배치 사용이 일반적
  * 적절한 배치 크기를 위해 훈련 데이터셋의 크기와 같게 하거나 하나의 배치로 훈련해 보는 등 다양한 테스트 시도

* 옵티마이저 및 손실 함수:
  * 일반적으로 stochatic gradient descent 많이 사용 (SGD)
  * 네트워크 구성에 따라 Adam, RMSPrpo 등도 좋은 성능을 보임.
  * 다양한 옵티마이저와 손실함수 적용 후 성능이 가장 좋은 것 선택
 
* 네트워크 구성
  * 최적의 네트워크 구성을 위해 구성을 변경해 가면서 테스트
  * 하나의 은닉층에 뉴런을 여러개 포함시키거나(넓은 네트워크), 네트워크 계층을 늘리되 뉴런 개수를 줄인다(깊은 네트워크)

 ## 앙상블을 이용한 네트워크 최적화
 * 간단한 모델을 두개 이상 섞어서 사용
 * 알고리즘 튜닝을 위한 성능 최적화 방법은 하이퍼파라미터에 대한 모든 경우의 수를 고려해야 하므로 모델 훈련이 수십, 수백번 필요할 수 있다.
 * 수많은 시행착오 필요
 
 ## 하이퍼 파라미터를 이용한 성능 최적화
 * 배치 정규화를 통한 성능 최적화 BATCHNORM
 * 드롭 아웃 DROP OUT
 * 조기 종료 EARLY STOPPING
 
  
  
