# 자연어전처리


## 토큰화 (Tokenization)
* 주어진 텍스트를 단어 또는 문자 단위로 자르는 것을 토큰화라고 함
* 파이썬의 기본 함수인 str.split() 메소드로 토큰화 가능
* 영어의 경우 토큰화를 위해 사용하는 도구로 spaCy와 NLTK

## 한국어 띄어쓰기 토큰화
* 영어의 경우는 띄어쓰기 단위로 토큰화를 해도, 단어들 각각의 구분이 꽤나 명확하기 때문에 토큰화 작업이 수월
  * "A Dog Run back corner near spare bedrooms" -> ['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']
* 하지만 한국어의 경우에는 띄어쓰기를 통한 토큰화 작업이 훨씬 까다로움 -> 한국어는 (어간) 조사, 접사 등으로 의미 표현 -> 단순 띄어쓰기로 단위를 나누면, 같은 단어가 다른 단어로 인식되어서 단어 집합 (vocabulary)의 크기가 불필요하게 커짐
    * 단어 집합(vocabulary) :중복을 제거한 텍스트의 총 단어의 집합 (set) 의미

## 형태소 토큰화
* 위와 같은 상황을 방지하기 위해서 한국어는 보편적으로 '형태소 분석기'로 토큰화
* 형태소 분석기 중에서 ★mecab 사용 / okt

```
colab mecab download 
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
%cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh
%cd ../
```

```
사용
from konlpy.tag import Mecab
tokenizer = Mecab()
tokenizer.nouns() 단어 토크나이징
tokenizer.morphs() 형태소 토크나이징
```

## 단어 집합 (Vocabulary) 생성
* 단어 집합 (Vocabulary) : 중복을 제거한 텍스트의 총 단어 집합 (set) 의미
  * 단어를 키(key)로, 단어에 대한 빈도수를 값(value)으로 저장된 사전형 데이터 
* 토큰화 ( + 불용어 처리) -> 단어 집합 생성 
  * 불용어 : 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어
 
 ```
 한국어 불용어
 stopwords=[
    '의','가','이','은','들','는','좀','잘',
    '걍','과','도','를','으로','자','에','와','한','하다']
 ```

```
vocab['단어'] -> 빈도수 반환
most_common() 상위 빈도수를 가진 주어진 수의 단어만을 리턴 
```

