# DO IT! BERT와 GPT로 배우는 자연어 처리

# 1장

## TRANSFER LEARNING : 특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법 (전이학습)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9cbe4004-ed48-4ff2-a990-69e2c8250bf5/Untitled.png)

- 기존(태스크를 처음부터 학습한) 모델보다 학습 속도가 빨라지고 새로운 태스크를 더 잘 수행하는 경향을 보임
    - BERT나 GPT등도 트랜스퍼 러닝 적용
    
- 태스크 1 = upstream
    - 다음 단어 맞히기, 빈칸 채우기 등 대규모 말뭉치의 문맥을 이해하는 과제
- 태스크 2 = downstream
    - 문서 분류, 개체명 인식 등 우리가 풀고자 하는 자연어 처리의 구체적인 문제

업스트림 태스크를 학습하는 과정을 pretrain이라고 한다.

# 업스트림 태스크

## 대표적인 업스트림 태스크 : 다음 단어 맞히기 (GPT 계열 모델)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e8711e91-0743-4083-86f4-5a04b306d651/Untitled.png)

티끌 모아 라는 문맥이 주어지고 학습 데이터 말뭉치에 티끌 모아 태산이라는 pharse가 많다고 하면 모델은 이를 바탕으로 다음에 올 단어를 태산으로 분류하도록 학습 

위 과정을 반복하면 해당 언어의 풍부한 문맥을 이해할 수 있게 된다.

이처럼 다음 단어 맞히기로 업스트림 태스크를 수행한 모델을 언어 모델 이라고 한다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5274912b-6869-4438-8daf-ad90b7607fab/Untitled.png)

(티끌 모아 다음 단어의 정답이 태산이라면 태산이라는 단어에 해당하는 확률은 높이고 나머지 단어들의 확률은 낮추는 방향으로 모델 전체를 업데이트 한다.)

## 빈칸 채우기 (BERT 계열)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cd1456ac-e6ab-42cb-9cff-be7de2bfc5dc/Untitled.png)

문장에서 빈칸을 만들고 해당 위치에 들어갈 단어가 무엇일지 맞히는 과정에서 학습된다.

빈칸채우기로 업스트림 태스크를 수행한 모델을 마스크 언어 모델 이라 한다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8a298e0e-056f-4774-8e6d-ae6eafbcd44e/Untitled.png)

### 업스트림 태스크의 대표 과제로 `다음단어 맞추기` 와 `빈칸 단어 채우기` 가 있다.

감정 분석처럼 사람이 만든 정답 데이터로 모델을 학습하는 방법을 지도학습 (supervised learning) 이라 한다.

이 방식은 데이터를 만드는 데 비용이 많이 들고 사람이 실수로 잘못된 레이블을 줄 수 있다.

업스트림태스크는 뉴스,웹문서,백과사전 등 글만 있으면 수작업 없이도 다량의 학습 데이터를 아주 싼값에 만들어 낼 수 있다.

이 처럼 데이터 내에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 자기지도 학습(self-supervised learning)이라 한다.

# 다운스트림 태스크

### 업스트림태스크 프리트레인의 목적 : 다운스트림 태스크를 잘하기 위함

다운스트림 태스크의 본질은 분류 이다. 

자연어를 입력받아 해당 입력이 어떤 범주에 해당하는지 확률 형태로 반환한다.

문장 생성을 제외한 대부분의 과제에서는 프리트레인을 마친 마스크 언어 모델(BERT 계열)을 사용한다.

다운스트림 태스크의 학습 방식은 fine tunning

### 다운스트림 태스크는 자연어 처리의 구체적인 과제들이다.

### 업스트림(프리트레인) 다운스트림(파인튜닝)

파인튜닝은 프리트레인을 마친 모델을 다운스트림 태스크에 맞게 업데이트하는 기법

문서 분류를 수행할 경우 프리트레인을 마친 BERT 모델 전체를 문서 분류 데이터로 업데이트

개체명 인식을 수행할 경우 BERT 모델 전체를 해당 데이터로 업데이트

## 문서 분류

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/faa32724-1a19-4117-854c-4b6dda99dda1/Untitled.png)

문서 분류 모델은 자연어(문서나 문장)을 입력받아 해당 입력이 어떤 범주(긍정,중립,부정 등)에 속하는지 그 확률 값을 반환

마스크 언어 모델 (노란색 박스) 위에 작은 모듈(초록색 박스 : 부정)을 하나 더 쌓아 문서 전체의 범주를 분류

CLS SEP는 각각 문장의 시작과 끝에 붙이는 특수한 TOKEN

### 자연어 추론

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3b12192b-c2c6-41f8-a960-f279380deb8b/Untitled.png)

자연어 추론 모델은 문장 2개를 입력받아 두 문장 사이의 관계가 참,거짓,중립 등 어떤 범주인지 그확률 값을 반환

### 개체명 인식

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4545b8b3-df03-4404-9a8d-f29cae21372b/Untitled.png)

단어별로 기관명,인명,지명등 개체명 범주에 속하는지 확률값을 반환

### 질의응답

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b593806e-5a20-44e5-aa65-539a02a2ad0f/Untitled.png)

자연어(질문+지문)을 입력받아 각 단어가 정답의 시작일 확률과 끝일 확률값을 반환

전체 단어 가운데 어떤 단어가 시작(초록색)인지 끝(빨간색)인지 분류 

### 문장생성

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5048754e-aed3-4a66-ba27-559183b6c345/Untitled.png)

문장생성 모델은 GPT 계열 언어 모델이 널리 쓰인다.

문장생성 모델은 자연어(문장)을 입력받아 어휘 전체에 대한 확률값을 반환함 이확률값은 입력된 문장 다음에 올 단어로 얼마나 적절한지를 나타내는 점수 이다.

프리트레인을 마친 언어 모델을 구조 변경 없이 그대로 사용해 문맥에 이어지는 적절한 다음 단어를 분류하는 방식 

## 그외에 전이학습 기법들

- **파인튜닝(fine-tuning)** : 다운스트림 태스크 데이터 전체를 사용합니다. 다운스트림 데이터에 맞게 모델 전체를 업데이트합니다.

파인튜닝 이외의 방식이 주목받고 있는 이유는 비용과 성능 때문입니다. 최근 언어 모델의 크기가 기하급수로 커져 파인튜닝 방식으로 모델 전체를 업데이트하려면 많은 비용이 든다. 그 뿐만 아니라 프롬프트 튜닝, 인컨텍스트 러닝으로 학습한 모델이 경쟁력 있는 태스크 수행 성능을 보일 때가 많습니다.

- **프롬프트 튜닝(prompt tuning)** : 다운스트림 태스크 데이터 전체를 사용합니다. 다운스트림 데이터에 맞게 모델 일부만 업데이트합니다.

- **인컨텍스트 러닝(in-context learning)** : 다운스트림 태스크 데이터의 일부만 사용합니다. 모델을 업데이트하지 않습니다.
    - **제로샷러닝(zero-shot learning)** : 다운스트림 태스크 데이터를 전혀 사용하지 않습니다. 모델이 바로 다운스트림 태스크를 수행합니다.
    - **원샷러닝(one-shot learning)** : 다운스트림 태스크 데이터를 1 건만 사용합니다. 모델은 1건의 데이터가 어떻게 수행되는지 참고한 뒤 다운스트림 태스크를 수행합니다.
    - **퓨샷러닝(few-shot learning)** : 다운스트림 태스크 데이터를 몇 건만 사용합니다. 모델은 몇 건의 데이터가 어떻게 수행되는지 참고한 뒤 바로 다운스트림 태스크를 수행합니다.
- 인컨텍스트 런닝 : 모델을 업데이트하지 않고도 다운스트림 태스크를 바로 수행할 수 있다는 장점이 있다.

# 학습 파이프라인

각종 설정값(하이퍼파라미터,러닝레이트,배치사이즈 등) →데이터 수집 → 전이학습 모델 준비 → 토크나이저 → 데이터로더 → 태스크 정의 → 모델학습
